{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>**An Analysis of the Correlates of War**  \n",
    "<center>By Wesley Day"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "War has been a terrible part of the human condition for at least thousands of years. The first half of the 20th century saw the entire world engulfed in war twice, causing widespread destruction and loss of life. The second half was also filled with war and looming over it all was the possibility of nuclear armageddon. The recent [war in Ukraine](https://www.cnn.com/world/europe/ukraine) and [growing hostility between the US and China](https://www.nytimes.com/article/us-china-tensions-explained.html) has made me, and I suspect many others, worry that another terrible war on the scale of the World Wars will happen in the next couple of decades."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I hope to find some correlates of war and in the process, show the data science lifecycle from start to finish. I found a great resource for this called the [Correlates of War Project](https://correlatesofwar.org/). On their website, they have several data sets that cover topics like interstate war, intrastate war, militarized interstate disputes, trade, religion, and more. All of the data sets were made to be compatible with each other, which makes our job a lot easier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, there are some definitions and clarifications I need to provide. An interstate war is a war between states. The requirements to be considered a state by the Correlates of War Project are \"membership in the League of Nations or the United Nations, or a population of at least 500,000 and recognition (through diplomatic missions) by two major powers\". Interstate war is the type of war we are going to be focusing on. A dyad is a set of two countries and a directed dyad is a set of two countries where order matters (i.e. USA and Canada is different from Canada and USA). We will be using directed dyads throughout this tutorial. In the datasets and in our analysis, countries/states will be classified by a country code (ccode) which is a unique 1-3 digit positive integer for each country. More information about this is provided in the second code block.  \n",
    "If you are interested in doing your own dive into the data or would like to know more about any of the data I use here (how it was collected, coded, etc.) I encourage you to visit the Correlates of War Project website linked above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by importing some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import chardet\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the datasets we are using, each country is associated with a country code.\n",
    "# Here, we are reading in the csv that contains these codes and creating\n",
    "# a dictionary so that we can access countries by their code rather than their name.\n",
    "# This is important because the names of countries have changed over the time period\n",
    "# we are looking at (e.g. USSR -> Russia, Ottoman Empire -> Turkey).\n",
    "# It is important to keep this in mind during our analysis as this will show up as\n",
    "# \"Russia\" having fought in WW2, for example, when really \"USSR\" fought in WW2.\n",
    "# This is okay for our purposes because we are not primarily concerned about\n",
    "# individual states but rather the quantitative things about states that are correlated with war.\n",
    "\n",
    "country_codes = pd.read_csv(\"correlates_of_war/COW-country-codes.csv\")\n",
    "country_codes = country_codes.drop_duplicates()\n",
    "country_codes = country_codes.set_index(\"CCode\")\n",
    "country_dict = country_codes.to_dict(\"index\")\n",
    "\n",
    "# The data sets that we are working with are very large. I ran into some problems\n",
    "# with the kernel crashing, so deleting the DataFrames when we're done with them should help.\n",
    "del country_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StateAbb': 'USA', 'StateNme': 'United States of America'}\n",
      "{'StateAbb': 'CAN', 'StateNme': 'Canada'}\n",
      "{'StateAbb': 'RUS', 'StateNme': 'Russia'}\n",
      "Not a valid country code\n"
     ]
    }
   ],
   "source": [
    "# Here we are checking that this worked correctly by plugging in some of\n",
    "# the country codes and making sure they are associated with the right country.\n",
    "\n",
    "print(country_dict[2]) # should be USA\n",
    "print(country_dict[20]) # should be Canada\n",
    "print(country_dict[365]) # should be Russia\n",
    "try:\n",
    "    print(country_dict[3]) # no country associated with this code, should throw a KeyError\n",
    "except KeyError:\n",
    "    print(\"Not a valid country code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to read in the rest of our csv files. Some of the files use Latin-1 encoding which\n",
    "# causes problems when we try read it in with pandas as pandas assumes UTF-8 by default. To make this simpler\n",
    "# I wrote a short function that will figure out the encoding. This isn't the most efficient, but it's\n",
    "# not too slow and it works.\n",
    "def get_encoding(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return chardet.detect(f.read())[\"encoding\"]\n",
    "\n",
    "filename = \"correlates_of_war/COW War Data/Dyadic-Interstate-War-Dataset/directed_dyadic_war.csv\"\n",
    "war = pd.read_csv(filename, encoding=get_encoding(filename))\n",
    "\n",
    "#filename = \"correlates_of_war/Diplomatic Exchange/Diplomatic_Exchange_2006v1.csv\"\n",
    "#diplomatic_exchanges = pd.read_csv(filename, encoding=get_encoding(filename))\n",
    "\n",
    "filename = \"correlates_of_war/Colonial Contiguity/contcol.csv\"\n",
    "colonial_contiguity = pd.read_csv(filename, encoding=get_encoding(filename))\n",
    "\n",
    "filename = \"correlates_of_war/Direct Contiguity/contdird.csv\"\n",
    "direct_contiguity = pd.read_csv(filename, encoding=get_encoding(filename))\n",
    "\n",
    "#filename = \"correlates_of_war/Formal Alliances/alliance_v4.1_by_dyad_yearly.csv\"\n",
    "#formal_alliances = pd.read_csv(filename, encoding=get_encoding(filename))\n",
    "\n",
    "filename = \"correlates_of_war/Militarized Interstate Disputes/dyadic_mid_4.02.csv\"\n",
    "mid = pd.read_csv(filename, encoding=get_encoding(filename))\n",
    "\n",
    "filename = \"correlates_of_war/National Material Capabilities/NMC-60-abridged/NMC-60-abridged.csv\"\n",
    "nmc = pd.read_csv(filename, encoding=get_encoding(filename))\n",
    "\n",
    "#filename = \"correlates_of_war/State System Membership/majors2016.csv\"\n",
    "#major_powers = pd.read_csv(filename, encoding=get_encoding(filename))\n",
    "\n",
    "filename = \"correlates_of_war/Territorial Change/tc2018.csv\"\n",
    "territorial_change = pd.read_csv(filename, encoding=get_encoding(filename))\n",
    "\n",
    "filename = \"correlates_of_war/Trade/Dyadic_COW_4.0.csv\"\n",
    "trade = pd.read_csv(filename, encoding=get_encoding(filename))\n",
    "\n",
    "#filename = \"correlates_of_war/World Religion/WRP_national.csv\"\n",
    "#religion = pd.read_csv(filename, encoding=get_encoding(filename))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Cleaning**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily for us, the Correlates of War Project has spent a lot of time making their data as easy to use as possible, which includes interpolating missing data sometimes or indicating that the data is missing otherwise. For example, -9 typically means missing data. I've looked through the codebooks that are provided with each dataset and can say that everywhere negative integers occur, it is meant to signify data is missing or not applicable, and so we can safely replace all instances of negative integers with NaN. We do this so that these negative numbers don't affect our analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the columns we don't need\n",
    "nmc = nmc.drop(columns=['stateabb', 'milex', 'milper', 'irst', 'pec', 'tpop', 'upop', 'version'])\n",
    "\n",
    "colonial_contiguity = colonial_contiguity.drop(columns=['dyad', 'statelab', 'dependl', 'statehab', 'dependh', 'version'])\n",
    "\n",
    "direct_contiguity = direct_contiguity.drop(columns=['dyad', 'state1ab', 'state2ab', 'version'])\n",
    "\n",
    "trade = trade.drop(columns=['importer1', 'importer2', 'smoothflow1', 'smoothflow2', 'smoothtotrade', 'spike1', 'spike2', \n",
    "                    'dip1', 'dip2', 'trdspike', 'tradedip', 'bel_lux_alt_flow1', 'bel_lux_alt_flow2', 'china_alt_flow1', 'china_alt_flow2',\n",
    "                    'source1', 'source2', 'version'])\n",
    "\n",
    "#diplomatic_exchanges = diplomatic_exchanges.drop(columns=['version'])\n",
    "\n",
    "territorial_change = territorial_change.drop(columns=['month', 'gaintype', 'procedur', 'entity', 'contgain', 'area', 'pop', 'portion', \n",
    "                                 'losetype', 'contlose', 'entry', 'exit', 'number', 'indep', 'conflict', 'version'])\n",
    "\n",
    "# go through the codebook again, probably want to use strtyr instead of year, might want to use some of these (if I'm using all those diplo ones\n",
    "# should definitely use a few of these)\n",
    "mid = mid.drop(columns=['disno', 'dyindex', 'namea', 'nameb', 'strtday', 'strtmnth', 'strtyr', 'endday', 'endmnth', 'endyear', 'settlmnt', \n",
    "                        'fatlev', 'highact', 'hihost', 'recip', 'noinit', 'notarg', 'sideaa', 'revstata', 'revtypea', 'fatleva', \n",
    "                        'highmcaa', 'hihosta', 'orignata', 'sideab', 'revstatb', 'revtypeb', 'fatlevb', 'highmcab', 'hihostb', \n",
    "                        'orignatb', 'rolea', 'roleb', 'dyad_rolea', 'dyad_roleb', 'durindx', 'duration', 'cumdurat', 'mid5hiact', \n",
    "                        'mid5hiacta', 'mid5hiactb', 'severity', 'severitya', 'severityb', 'ongo2014', 'new', 'change', 'changetype_1',\n",
    "                        'changetype_2', 'dyad', 'abbreva', 'abbrevb', 'lastobs', 'newar', 'outcome', 'war'])\n",
    "\n",
    "war = war.drop(columns=['warnum', 'disno', 'dyindex', 'warstrtmnth', 'warstrtday', 'warendmnth', 'warenday', 'warendyr', 'warolea', \n",
    "                        'waroleb', 'wardyadroleb', 'batdtha', 'batdthb', 'batdths', 'outcomea', 'changes_1', 'changes_2', 'durindx', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The datasets typically use -9 to mean missing value, but some use -8. Regardless,\n",
    "# negative integers are codes for something and not actual data so we can safely replace\n",
    "# these with NaN.\n",
    "def replace_missing(df):\n",
    "    return df.applymap((lambda x: np.nan if x == -9 or x == -8 else x))\n",
    "\n",
    "war = replace_missing(war)\n",
    "#diplomatic_exchanges = replace_missing(diplomatic_exchanges)\n",
    "colonial_contiguity = replace_missing(colonial_contiguity)\n",
    "direct_contiguity = replace_missing(direct_contiguity)\n",
    "#formal_alliances = replace_missing(formal_alliances)\n",
    "#mid = replace_missing(mid)\n",
    "nmc = replace_missing(nmc)\n",
    "trade = replace_missing(trade)\n",
    "#religion = replace_missing(religion)\n",
    "territorial_change = replace_missing(territorial_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to do something about missing data, if I want to just leave it, \n",
    "# need to explain why, could maybe do interpolation here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what do we want to do"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "want to do more checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are calculating the percent difference in military capabilities between each\n",
    "# country and every other country for each year in the dataset (1816-2016).\n",
    "def percent_diff(row):\n",
    "    rest = nmc[(nmc['ccode'] != row['ccode']) & (nmc['year'] == row['year'])]\n",
    "    cinc_diff = row['cinc'] - rest['cinc'].values\n",
    "    cinc_sum = row['cinc'] + rest['cinc'].values\n",
    "    percent_diff = (cinc_diff / cinc_sum / 2) * 100\n",
    "    ccode1 = np.full_like(rest['ccode'].values, row['ccode'])\n",
    "    year = np.full_like(ccode1, row['year'])\n",
    "    return pd.DataFrame(\n",
    "        {'year': year, 'ccode1': ccode1, 'ccode2': rest['ccode'].values, 'capability_percent_diff': percent_diff})\n",
    "\n",
    "percent_diff_df = nmc.apply(percent_diff, axis=1)\n",
    "df = pd.concat(percent_diff_df.tolist(), axis=0, ignore_index=True)\n",
    "\n",
    "del percent_diff_df, nmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:916: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  key_col = Index(lvals).where(~mask_left, rvals)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1912350\n",
      "1912350\n"
     ]
    }
   ],
   "source": [
    "# Adding direct contiguity, both datasets cover 1816-2016. conttype:\n",
    "# 1 = separated by a land or river border\n",
    "# 2 = separated by <= 12 miles of water\n",
    "# 3 = separated by (12, 24] miles of water or less\n",
    "# 4 = separated by (24, 150] miles of water or less\n",
    "# 5 = separated by (150, 400] miles of water or less\n",
    "# 6 = separated by > 400 miles of water or no direct route (e.g. Afghanistan and Kazakhstan)\n",
    "\n",
    "# In this dataset, they do not have an entry for every year. Instead, they have a begin year and an end year.\n",
    "# Here we are exploding the DataFrame to have an entry for each year between the begin and end year.\n",
    "# This creates some duplicate rows, which we drop.\n",
    "colonial_contiguity['year'] = \\\n",
    "    [range(begin, end + 1) for begin, end in zip(colonial_contiguity['begin'], colonial_contiguity['end'])]\n",
    "colonial_contiguity = colonial_contiguity.explode('year')\n",
    "colonial_contiguity = colonial_contiguity.drop(columns=['begin', 'end'])\n",
    "colonial_contiguity = colonial_contiguity.drop_duplicates()\n",
    "colonial_contiguity = colonial_contiguity.rename(columns={'statelno': 'ccode1', 'statehno': 'ccode2'})\n",
    "\n",
    "# colonial_contiguity only has one row per dyad, rather than two rows like the\n",
    "# DataFrame we're building. To fix this, we just make a copy of colonial_contiguity and \n",
    "# switch the names of the columns, then concatenate them together.\n",
    "colonial_contiguity = pd.concat(\n",
    "    [colonial_contiguity, colonial_contiguity.rename(columns={'ccode1': 'ccode2', 'ccode2': 'ccode1'})], ignore_index=True)\n",
    "\n",
    "direct_contiguity = direct_contiguity.rename(columns={'state1no': 'ccode1', 'state2no': 'ccode2'})\n",
    "\n",
    "# Now we are merging our two contiguity DataFrames based on ccode1, ccode2, and year. This results\n",
    "# in two conttype columns, conttype_1 and conttype_2 (that's what the suffixes= is for), conttype_1\n",
    "# is the value from conttype in direct_contiguity and conttype_2 is the one from colonial_contiguity.\n",
    "direct_contiguity = pd.merge(\n",
    "    direct_contiguity, colonial_contiguity, on=['ccode1', 'ccode2', 'year'], how='outer', suffixes=('_1', '_2'))\n",
    "\n",
    "# The merge will fill in NaN in conttype_1 and conttype_2 when there was not a corresponding entry\n",
    "# in the other DataFrame. For example, in 1816 USA and UK did not have direct contiguity, but they did\n",
    "# have a colonial contiguity through Canada. So, in this example conttype_1 would be NaN and conttype_2 would be 1.\n",
    "# We are replacing these NaNs with 6, indicating no contiguity, then finding our true conttype value by taking the\n",
    "# minimum of the two conttypes.\n",
    "direct_contiguity = direct_contiguity.fillna(6)\n",
    "direct_contiguity['conttype'] = direct_contiguity[['conttype_1', 'conttype_2']].min(axis=1)\n",
    "direct_contiguity = direct_contiguity.drop(columns=['conttype_1', 'conttype_2'])\n",
    "\n",
    "# There are multiple rows where year, ccode1, and ccode2 match but with different conttypes.\n",
    "# So we have to find the minimum of these, and then drop the rest. To do this, we group by\n",
    "# year, ccode1, and ccode2, then sort them so that the smallest value is the first row.\n",
    "# We can then drop rows where year, ccode1, and ccode2 are equal only keeping the first as\n",
    "# we know this the smallest one. This is a long operation, it takes ~40 seconds on my computer.\n",
    "direct_contiguity = direct_contiguity.groupby(['year', 'ccode1', 'ccode2']).apply(lambda x: x.sort_values('conttype'))\n",
    "direct_contiguity = direct_contiguity.reset_index(drop=True)\n",
    "direct_contiguity = direct_contiguity.drop_duplicates(subset=['year', 'ccode1', 'ccode2'], keep='first')\n",
    "\n",
    "# Adding conttype to our DataFrame.\n",
    "# We can do outer or left merge here, as df already contains every directed dyad for every year.\n",
    "# I use outer just to be safe, and then we can check that no new rows were added (indicating that\n",
    "# we really do already have every directed dyad).\n",
    "print(len(df))\n",
    "df = pd.merge(df, direct_contiguity, on=['ccode1', 'ccode2', 'year'], how='outer')\n",
    "df['conttype'] = df['conttype'].fillna(6)\n",
    "print(len(df))\n",
    "\n",
    "del direct_contiguity, colonial_contiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1912350\n",
      "1912350\n"
     ]
    }
   ],
   "source": [
    "# Trade dataset is from 1870-2014\n",
    "# flow1 = imports of ccode1 from ccode2 in millions of 2014 USD\n",
    "# flow2 = exports of ccode1 to ccode2 in millions of 2014 USD\n",
    "\n",
    "# Like with colonial_contiguity, this dataset only has one row for each dyad rather than two like we want.\n",
    "# Fixing it in the exact same way as before, except this time we also need to switch flow1 and flow2.\n",
    "trade = pd.concat([trade, trade.rename(columns={'ccode1': 'ccode2', 'ccode2': 'ccode1', 'flow1': 'flow2', 'flow2': 'flow1'})])\n",
    "\n",
    "# Calculating imports/export each country has with another by its relative amount\n",
    "# to that country's total trade in a given year as a percentage. E.g. if the US exports\n",
    "# a total of $1 billion of goods in a year and exports $500 million to Canada\n",
    "# then USA's export_percent with Canada is 50.\n",
    "grouped = trade.groupby(['ccode1', 'year'])[['flow1', 'flow2']].transform('sum')\n",
    "trade['flow1_sum'] = grouped['flow1']\n",
    "trade['flow2_sum'] = grouped['flow2']\n",
    "trade['import_percent'] = (trade['flow1'] / trade['flow1_sum']) * 100\n",
    "trade['export_percent'] = (trade['flow2'] / trade['flow2_sum']) * 100\n",
    "trade = trade.drop(columns=['flow1', 'flow1_sum', 'flow2', 'flow2_sum'])\n",
    "\n",
    "# Adding these two columns to our DataFrame.\n",
    "# Again, we'll use an outer merge to be safe, and we can check that no new rows were added.\n",
    "print(len(df))\n",
    "df = pd.merge(df, trade, on=['ccode1', 'ccode2', 'year'], how='outer')\n",
    "print(len(df))\n",
    "\n",
    "del trade, grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Diplomatic Exchanges. Data is from 1817-2005, but in increments ranging from 3-10 years (typically 5)\n",
    "# # The data is already fine as is, so we can just merge it.\n",
    "# print(len(df))\n",
    "# df = pd.merge(df, diplomatic_exchanges, on=['ccode1', 'ccode2', 'year'], how='outer')\n",
    "# print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Territorial Change (1816-2008)\n",
    "# We are going to keep things simple and make this binary. If a country gained land from another country then\n",
    "# we'll put a 1 in the gained_territory column, 0 otherwise. If a county lost land from another country then\n",
    "# we'll put a 1 in the lost_territory column, 0 otherwise.\n",
    "\n",
    "# Dropping rows that contain NaN, as if we don't know the year or who gained or lost territory, the data is worthless to us.\n",
    "territorial_change = territorial_change.dropna()\n",
    "\n",
    "# Here we are creating a new column in territorial_change and setting all those values to true\n",
    "# and then copying territorial_change and renaming some of the columns. We then merge these with\n",
    "# our main DataFrame which results in True everywhere there was a match between year, ccode1, and ccode2,\n",
    "# and NaN everywhere else. We then fill in NaN with False in the two new columns we added.\n",
    "territorial_change = territorial_change.rename(columns={'gainer': 'ccode1', 'loser': 'ccode2'})\n",
    "territorial_change['gained_territory'] = 1\n",
    "losers = territorial_change.rename(columns={'ccode1': 'ccode2', 'ccode2': 'ccode1', 'gained_territory': 'lost_territory'})\n",
    "\n",
    "# An outer merge here would cause about 300 new rows to be added. Since all of our\n",
    "# previous data have shared year/ccode1/ccode2 combinations, I'm going to do a left merge\n",
    "# as filling in all the NaNs for the new rows would be impossible.\n",
    "df = pd.merge(df, territorial_change, on=['ccode1', 'ccode2', 'year'], how='left')\n",
    "df = pd.merge(df, losers, on=['ccode1', 'ccode2', 'year'], how='left')\n",
    "df = df.drop_duplicates()\n",
    "df[['gained_territory', 'lost_territory']] = df[['gained_territory', 'lost_territory']].fillna(0)\n",
    "\n",
    "del territorial_change, losers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data won't really help with our analysis. War is going to be a binary variable,\n",
    "# so right now this will just tell us how correlated a change of territory is with war\n",
    "# for the same year. But what I'm wondering is if a change in territory in the past makes\n",
    "# war more likely (e.g. Germany losing land to France in WW1 and trying to get it back in WW2). \n",
    "# I am going to pick a somewhat arbitrary cutoff of territory change in the last 60 years. \n",
    "# My reasoning being that this is a long enough time where only a small minority will be \n",
    "# able to remember a time when that territory was/wasn't a part of their country.\n",
    "\n",
    "# To accomplish this, we are sorting the DataFrame by year. Then, we create\n",
    "# groups based on directed dyads and take the max of the last 60 years.\n",
    "# shift() moves everything up a row, ensuring that we are only looking into the\n",
    "# past and not the current year. min_periods = 1 ensures that we take as many rows as \n",
    "# we can when there are not 60 previous rows (e.g. in 1820). Finally we use max()\n",
    "# as there are only 1's and 0's in the gained/lost_territory columns, giving us another\n",
    "# binary variable in gained/lost_in_past. We then reset the index to keep things organized.\n",
    "df = df.sort_values('year').reset_index(drop=True)\n",
    "df['gained_in_past'] = df.groupby(['ccode1', 'ccode2'])['gained_territory'].apply(\n",
    "    lambda grp: grp.shift().rolling(61, min_periods=1).max()).reset_index(drop=True)\n",
    "df['gained_in_past'] = df['gained_in_past'].fillna(0)\n",
    "df['lost_in_past'] = df.groupby(['ccode1', 'ccode2'])['lost_territory'].apply(\n",
    "    lambda grp: grp.shift().rolling(61, min_periods=1).max()).reset_index(drop=True)\n",
    "df['lost_in_past'] = df['lost_in_past'].fillna(0)\n",
    "\n",
    "# We can now drop the gained/lost_territory columns.\n",
    "df = df.drop(columns=['gained_territory', 'lost_territory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1912350\n",
      "1913420\n"
     ]
    }
   ],
   "source": [
    "# Militarized Interstate Disputes (1816-2014)\n",
    "mid = mid.rename(columns={'statea': 'ccode1', 'stateb': 'ccode2'})\n",
    "mid['mid'] = 1\n",
    "\n",
    "# Here we expect duplicate rows. We'll get a row for each MID that occured in a year.\n",
    "# We'll use this in the next code block to combine this into a new column for the number\n",
    "# of MIDs that occured in a year.\n",
    "print(len(df))\n",
    "df = pd.merge(df, mid, on=['year', 'ccode1', 'ccode2'], how='outer')\n",
    "print(len(df))\n",
    "\n",
    "del mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df.duplicated(subset=['year', 'ccode1', 'ccode2'], keep=False)][['year', 'ccode1', 'ccode2', 'mid']]\n",
    "df2['mids'] = df2.groupby(['year', 'ccode1', 'ccode2'])['mid'].transform('count')\n",
    "df2 = df2.drop(columns=['mid']).drop_duplicates()\n",
    "df = df.drop(columns=['mid'])\n",
    "df = pd.merge(df, df2, on=['year', 'ccode1', 'ccode2'], how='left')\n",
    "df = df.drop_duplicates()\n",
    "df['mids'] = df['mids'].fillna(0)\n",
    "\n",
    "del df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MIDs dataset added some rows that did not have a corresponding row in our DataFrame.\n",
    "# Since we don't have any other data in those rows, we'll just have to drop them.\n",
    "df = df.dropna(subset=['capability_percent_diff', 'conttype', 'gained_in_past', 'lost_in_past'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we want a rolling count of MIDs from the past so many years.\n",
    "# I'm going to create several columns counting varying years into the past\n",
    "# and during our analysis we'll see how long into the past MIDs seem to matter\n",
    "# when trying to predict war.\n",
    "df = df.sort_values('year').reset_index(drop=True)\n",
    "\n",
    "df['mids_past5'] = df.groupby(['ccode1', 'ccode2'])['mids'].apply(\n",
    "    lambda grp: grp.shift().rolling(6, min_periods=1).sum()).reset_index(drop=True)\n",
    "df['mids_past5'] = df['mids_past5'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccode1           560\n",
      "ccode2           540\n",
      "year            1975\n",
      "c1_initiated       1\n",
      "at_war             1\n",
      "Name: 532, dtype: int64\n",
      "ccode1           560\n",
      "ccode2           540\n",
      "year            1975\n",
      "c1_initiated       0\n",
      "at_war             1\n",
      "Name: 533, dtype: int64\n",
      "ccode1           540\n",
      "ccode2           560\n",
      "year            1975\n",
      "c1_initiated       0\n",
      "at_war             1\n",
      "Name: 530, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# War (1816-2010)\n",
    "# The dataset already has directed dyads for every year (e.g. war between a and b that lasted from 1999-2000 has 4 entries)\n",
    "\n",
    "war = war.rename(columns={'statea': 'ccode1', 'stateb': 'ccode2', 'warstrtyr': 'year', 'wardyadrolea': 'c1_initiated'})\n",
    "war['at_war'] = 1\n",
    "\n",
    "# There are some duplicated rows because we are using the start year of each war.\n",
    "# In the original dataset, there is an entry for every year the war is going on.\n",
    "# So if a war lasted for 3 years, we have 3 rows that are the same.\n",
    "war = war.drop_duplicates()\n",
    "\n",
    "# 3 in the original column ('wardyadrolea') indicates that ccode1 was the target\n",
    "war['c1_initiated'] = war['c1_initiated'].replace(3, 0)\n",
    "war = war.reset_index(drop=True)\n",
    "\n",
    "# There is one instance of duplicate row with different c1_initiated values.\n",
    "print(war.iloc[532])\n",
    "print(war.iloc[533])\n",
    "\n",
    "# To resolve this, I looked at the what the value was for the reverse country codes\n",
    "# and kept the row with opposite value.\n",
    "print(war.iloc[530])\n",
    "war = war.drop(533)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1912350\n",
      "1912350\n"
     ]
    }
   ],
   "source": [
    "# Merging with our DataFrame, dropping duplicates, and seeing if anything was added.\n",
    "print(len(df))\n",
    "df = pd.merge(df, war, on=['ccode1', 'ccode2', 'year'], how='outer')\n",
    "df = df.drop_duplicates()\n",
    "df['at_war'] = df['at_war'].fillna(0)\n",
    "print(len(df))\n",
    "\n",
    "del war"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like with territorial change, we're going to make a column for if two countries were at war \n",
    "# in the past 60 years. We're doing this in the exact same way.\n",
    "df = df.sort_values('year').reset_index(drop=True)\n",
    "df['war_in_past'] = df.groupby(['ccode1', 'ccode2'])['at_war'].apply(\n",
    "    lambda grp: grp.shift().rolling(61, min_periods=1).max()).reset_index(drop=True)\n",
    "df['war_in_past'] = df['war_in_past'].fillna(0)\n",
    "df = df.drop(columns=['at_war'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final bit of data cleaning before we start the analysis. We're filling in NaN values in\n",
    "# c1_inititated to 0 because NaN means there was not an entry in the war DataFrame corresponding\n",
    "# to that year, meaning that a war did not begin between these two countries during this year.\n",
    "df['c1_initiated'] = df['c1_initiated'].fillna(0)\n",
    "\n",
    "# Making sure all our variables that should be integers are.\n",
    "df[['year', 'ccode1', 'ccode2', 'conttype', 'gained_in_past', 'lost_in_past', 'c1_initiated', 'war_in_past', 'mids_past5']] = \\\n",
    "    df[['year', 'ccode1', 'ccode2', 'conttype', 'gained_in_past', 'lost_in_past', 'c1_initiated', 'war_in_past', 'mids_past5']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year                            0\n",
       "ccode1                          0\n",
       "ccode2                          0\n",
       "capability_percent_diff         0\n",
       "conttype                        0\n",
       "import_percent             449653\n",
       "export_percent             449699\n",
       "gained_in_past                  0\n",
       "lost_in_past                    0\n",
       "mids                            0\n",
       "mids_past5                      0\n",
       "c1_initiated                    0\n",
       "war_in_past                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# War data is only until 2010, since this is our dependent variable, we can drop everything after this year.\n",
    "# I also want to do an analysis with MIDs as the dependent variable, \n",
    "# so I'm going to keep a copy of that goes to 2014.\n",
    "df_2014 = df.copy()[df['year'] <= 2014]\n",
    "df = df[df['year'] <= 2010]\n",
    "\n",
    "# We have a lot of NaNs. A lot of these are caused by the trade dataset being between 1870-2014 rather than\n",
    "# 1816-2016. So we can create a new DataFrame containing only these rows to start.\n",
    "\n",
    "df_1870 = df[df['year'] >= 1870]\n",
    "# Checking if we have more NaNs\n",
    "df_1870.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Need more explanation #\n",
    "#########################\n",
    "# As you can see there are still a lot of missing data in the trade columns.\n",
    "df_1870 = df_1870.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000932\n",
      "         Iterations 15\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:           c1_initiated   No. Observations:              1126205\n",
      "Model:                          Logit   Df Residuals:                  1126191\n",
      "Method:                           MLE   Df Model:                           13\n",
      "Date:                Thu, 11 May 2023   Pseudo R-squ.:                  0.1628\n",
      "Time:                        17:18:23   Log-Likelihood:                -1050.0\n",
      "converged:                       True   LL-Null:                       -1254.1\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.086e-79\n",
      "===========================================================================================\n",
      "                              coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "Intercept                  36.7989      5.310      6.930      0.000      26.391      47.207\n",
      "C(conttype)[T.2]           -0.9498      1.012     -0.939      0.348      -2.933       1.033\n",
      "C(conttype)[T.3]           -0.4206      1.015     -0.414      0.679      -2.409       1.568\n",
      "C(conttype)[T.4]           -0.3529      0.447     -0.789      0.430      -1.230       0.524\n",
      "C(conttype)[T.5]           -0.1334      0.363     -0.367      0.714      -0.845       0.579\n",
      "C(conttype)[T.6]           -2.0071      0.275     -7.298      0.000      -2.546      -1.468\n",
      "year                       -0.0226      0.003     -8.272      0.000      -0.028      -0.017\n",
      "capability_percent_diff    -0.0005      0.003     -0.171      0.864      -0.006       0.005\n",
      "import_percent              0.0157      0.010      1.493      0.135      -0.005       0.036\n",
      "export_percent             -0.0401      0.016     -2.535      0.011      -0.071      -0.009\n",
      "gained_in_past              0.6428      0.267      2.406      0.016       0.119       1.167\n",
      "lost_in_past                0.4975      0.263      1.889      0.059      -0.019       1.014\n",
      "war_in_past                 1.1644      0.264      4.411      0.000       0.647       1.682\n",
      "mids_past5                  0.2273      0.041      5.490      0.000       0.146       0.308\n",
      "===========================================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.90 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "model_1870 = sm.formula.logit(\n",
    "    'c1_initiated ~ year + capability_percent_diff + C(conttype) + import_percent + export_percent + gained_in_past + lost_in_past + war_in_past + mids_past5', \n",
    "    data=df_1870).fit()\n",
    "print(model_1870.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001396\n",
      "         Iterations 14\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:           c1_initiated   No. Observations:              1685370\n",
      "Model:                          Logit   Df Residuals:                  1685358\n",
      "Method:                           MLE   Df Model:                           11\n",
      "Date:                Thu, 11 May 2023   Pseudo R-squ.:                  0.1662\n",
      "Time:                        17:18:44   Log-Likelihood:                -2352.1\n",
      "converged:                       True   LL-Null:                       -2820.9\n",
      "Covariance Type:            nonrobust   LLR p-value:                4.892e-194\n",
      "===========================================================================================\n",
      "                              coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "Intercept                  20.5388      2.097      9.794      0.000      16.429      24.649\n",
      "C(conttype)[T.2]           -0.2826      0.419     -0.675      0.500      -1.103       0.538\n",
      "C(conttype)[T.3]           -0.4104      0.587     -0.700      0.484      -1.560       0.740\n",
      "C(conttype)[T.4]           -0.5249      0.271     -1.938      0.053      -1.056       0.006\n",
      "C(conttype)[T.5]           -0.7710      0.297     -2.597      0.009      -1.353      -0.189\n",
      "C(conttype)[T.6]           -2.3744      0.161    -14.778      0.000      -2.689      -2.059\n",
      "year                       -0.0141      0.001    -12.757      0.000      -0.016      -0.012\n",
      "capability_percent_diff     0.0028      0.002      1.636      0.102      -0.001       0.006\n",
      "gained_in_past              0.2810      0.186      1.510      0.131      -0.084       0.646\n",
      "lost_in_past                0.3669      0.184      1.992      0.046       0.006       0.728\n",
      "war_in_past                 1.4382      0.175      8.240      0.000       1.096       1.780\n",
      "mids_past5                  0.2083      0.029      7.190      0.000       0.152       0.265\n",
      "===========================================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.83 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "model = sm.formula.logit(\n",
    "    'c1_initiated ~ year + capability_percent_diff + C(conttype) + gained_in_past + lost_in_past + war_in_past + mids_past5', \n",
    "    data=df).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   mids   R-squared:                       0.013\n",
      "Model:                            OLS   Adj. R-squared:                  0.013\n",
      "Method:                 Least Squares   F-statistic:                     2431.\n",
      "Date:                Thu, 11 May 2023   Prob (F-statistic):               0.00\n",
      "Time:                        17:18:54   Log-Likelihood:             2.9251e+06\n",
      "No. Observations:             1836690   AIC:                        -5.850e+06\n",
      "Df Residuals:                 1836679   BIC:                        -5.850e+06\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===========================================================================================\n",
      "                              coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "Intercept                  -0.0029      0.002     -1.456      0.145      -0.007       0.001\n",
      "C(conttype)[T.2]            0.0220      0.001     24.876      0.000       0.020       0.024\n",
      "C(conttype)[T.3]           -0.0134      0.001    -14.241      0.000      -0.015      -0.012\n",
      "C(conttype)[T.4]           -0.0106      0.000    -24.777      0.000      -0.011      -0.010\n",
      "C(conttype)[T.5]           -0.0116      0.000    -29.832      0.000      -0.012      -0.011\n",
      "C(conttype)[T.6]           -0.0146      0.000    -63.705      0.000      -0.015      -0.014\n",
      "year                     8.815e-06   1.01e-06      8.754      0.000    6.84e-06    1.08e-05\n",
      "capability_percent_diff -4.509e-19   9.82e-07  -4.59e-13      1.000   -1.92e-06    1.92e-06\n",
      "gained_in_past              0.0053      0.000     13.157      0.000       0.005       0.006\n",
      "lost_in_past                0.0053      0.000     13.157      0.000       0.005       0.006\n",
      "war_in_past                 0.0357      0.000     96.724      0.000       0.035       0.036\n",
      "==============================================================================\n",
      "Omnibus:                  5704178.236   Durbin-Watson:                   1.958\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):     562785105021.786\n",
      "Skew:                          48.681   Prob(JB):                         0.00\n",
      "Kurtosis:                    2713.060   Cond. No.                     1.08e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.08e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "lin_model = sm.formula.ols(\n",
    "    'mids ~ year + capability_percent_diff + C(conttype) + gained_in_past + lost_in_past + war_in_past', \n",
    "    data=df_2014).fit()\n",
    "print(lin_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   mids   R-squared:                       0.012\n",
      "Model:                            OLS   Adj. R-squared:                  0.012\n",
      "Method:                 Least Squares   F-statistic:                     1166.\n",
      "Date:                Thu, 11 May 2023   Prob (F-statistic):               0.00\n",
      "Time:                        17:19:02   Log-Likelihood:             1.7750e+06\n",
      "No. Observations:             1126205   AIC:                        -3.550e+06\n",
      "Df Residuals:                 1126192   BIC:                        -3.550e+06\n",
      "Df Model:                          12                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===========================================================================================\n",
      "                              coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "Intercept                   0.0051      0.004      1.139      0.255      -0.004       0.014\n",
      "C(conttype)[T.2]            0.0329      0.001     29.051      0.000       0.031       0.035\n",
      "C(conttype)[T.3]           -0.0118      0.001    -10.308      0.000      -0.014      -0.010\n",
      "C(conttype)[T.4]           -0.0118      0.001    -22.449      0.000      -0.013      -0.011\n",
      "C(conttype)[T.5]           -0.0113      0.000    -24.328      0.000      -0.012      -0.010\n",
      "C(conttype)[T.6]           -0.0156      0.000    -53.772      0.000      -0.016      -0.015\n",
      "year                     5.293e-06   2.25e-06      2.357      0.018    8.92e-07    9.69e-06\n",
      "capability_percent_diff -2.661e-06   1.32e-06     -2.014      0.044   -5.25e-06   -7.09e-08\n",
      "import_percent             -0.0001   1.49e-05     -7.868      0.000      -0.000   -8.78e-05\n",
      "export_percent          -1.329e-07   1.38e-05     -0.010      0.992   -2.73e-05     2.7e-05\n",
      "gained_in_past              0.0056      0.000     11.779      0.000       0.005       0.007\n",
      "lost_in_past                0.0046      0.000      9.941      0.000       0.004       0.006\n",
      "war_in_past                 0.0282      0.000     65.209      0.000       0.027       0.029\n",
      "==============================================================================\n",
      "Omnibus:                  3524759.615   Durbin-Watson:                   1.985\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):     386479308821.923\n",
      "Skew:                          49.708   Prob(JB):                         0.00\n",
      "Kurtosis:                    2871.133   Cond. No.                     1.87e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.87e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "lin_model_1870 = sm.formula.ols(\n",
    "    'mids ~ year + capability_percent_diff + C(conttype) + import_percent + export_percent + gained_in_past + lost_in_past + war_in_past', \n",
    "    data=df_1870).fit()\n",
    "print(lin_model_1870.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Ideas\n",
    "\n",
    "# Feature selection: You can use ML algorithms to determine which features (or predictor variables) are the most \n",
    "# important for predicting the binary response variable c1_initiated. This can help you identify the key factors \n",
    "# that contribute to the likelihood of war initiation.\n",
    "\n",
    "# Clustering: You can use clustering algorithms to group countries together based on their similarities in terms \n",
    "# of the predictor variables. This can help you identify groups of countries that are more likely to engage in wars, \n",
    "# and potentially identify underlying factors that contribute to these similarities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
